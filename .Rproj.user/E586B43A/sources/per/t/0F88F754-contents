
# Nested Within Groups {#wGroups}

Lynette H. Bikos, PhD, ABPP, and Kiet D. Huynh, PhD
Co-Authors

[Screencasted Lecture Link](link here) 
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
```

```{r eliminates scientific notation}
options(scipen=999)#eliminates scientific notation
```

This chapter provides an introduction to multilevel modeling (MLM). Known by a variety of names, MLM offers researchers the ability to manage data that is *nested* within groups (cross-sectional),  within persons (longitudinal), or both. MLM is complex and powerful. This chapter will provide an introduction and worked example of MLM when data is collected in groups (churches). At the risk of oversimplification, my goal is to make it as accessible as possible. To that end, the chapter and lecture will err on the side of application. If you are interested in the more technical details of this procedure, there are tremendous resources that far exceed my capacity and the purpose of this OER [e.g., @bryk_hierarchical_1992]. 

At the outset of this series of chapters on MLM, let me share with you why I get so excited about this statistical approach.  Remember ANOVA?  And its assumptions?  Among these were assumptions of *balanced designs* (i.e., equal cell sizes), *independence* (i.e., unless using repeated measures ANOVA, participants could not be related/connected in other ways), and to rule out counfounds, *randomly assigned* to treatment conditions. Unless the data to be analyzed comes from an experiment, these are difficult conditions to meet.  When we are conducting cross-sectional research where there is clear nesting in groups (e.g., teams, classrooms) we are no longer bound by these restrictive assumptions. Presuming there is an adequate sample ([@bell_how_2014] suggested a minimum of 10 clusters with 5 members each),even the group size can vary. Of course there are other benefits and challenges that we will address throughout the series of chapters.

## Navigating this Lesson

There is about # hour and ## minutes of lecture.  If you work through the materials with me it would be plan for an additional TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReC_CPA) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Learning objectives from this lecture include the following:

* one
* two
* three
  * three.1
  * three.2
  * three.3

### Planning for Practice

In this chapter we offer two suggestions for practice.  Each are graded in complexity. At a minimum, we recommend that you analyzea multi-level model that contains one level-1 (within-group) predictor, one level-2 (between groups) predictor, and their interaction.

* Rework the problem in the chapter by changing the random seed in the code that simulates the data.  This should provide very minor changes to the data, but the results will likely be very similar. 
* The research vignette analyzes a number of variables, simultaneously. We selected only two for the example.  Swap out one or more variables in the multi-level model and compare your solution to the one in the chapter (and/or oNe you mimicked in the journal article). If you wish to increase your probability of finding statistically significant effects, look for hints in Table 2 of the [@lefevor_homonegativity_2020] research article that sources the vignettes by selecting a variable(s) with a significant relationship with your DV.
* Conduct a multi-level model with data to which you have access. This could include data you simulate on your own or from a published article.

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). *Applied multiple regression/correlation analysis for the behavioral sciences, 3rd ed.* Lawrence Erlbaum Associates Publishers
* Enders, C. K., & Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. *Psychological Methods, 12*(2), 121-138. doi:10.1037/1082-989X.12.2.121
* McCoach, D. B., & Adelson, J. L. (2010). Dealing with dependence (Part I): Understanding the effects of clustered data. *Gifted Child Quarterly, 54*(2), 152–155. https://doi-org.ezproxy.spu.edu/10.1177/0016986210363076
* McCoach, D. B. (2010).  Dealing with dependence (Part II):  A gentle introduction to hierarchical linear modeling. *Gifted Child Quarterly, 54*(3), 252-256. doi: 10.1177/0016986210373475 

### Packages

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r Install Packages for THIS SPECIFIC CHAPTER, include=FALSE}
#will install the package if not already installed
if(!require(lme4)){install.packages("lme4")}
if(!require(sjstats)){install.packages("sjstats")}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(psych)){install.packages("psych")}
if(!require(lmerTest)){install.packages("lmerTest")}
if(!require(robumeta)){install.packages("robumeta")}
if(!require(sjstats)){install.packages("sjstats")}
if(!require(PerformanceAnalytics)){install.packages("PerformanceAnalytics")}
```

## Multilevel Modeling:  Nested within Groups

### The dilemma of aggregation and disaggregation

It was the 1980s and researchers were studying group attitudes and were confused about how to analyze the data [@singer_applied_2003].They ran into difficulties with *aggregation* and asked, "Do we aggregate the data" by summing individuals within groups (i.e., giving everyone in the group the same score)?" Or, "Do we disaggregate the data" by ignoring group membership and analyzing the individual cases.

Problems with aggregation (using group means) include:

* Regression equations describe the relationship of means of predictors in individual clusters to the mean of the dependent variable in those clusters.
* There is a decrease in variability regarding the the ability to explain what is going on with the dependent variable.
* It can be misleading to generalize from the group level variable to the individual.  This is termed the *ecological fallacy* (also known as the *Robinson Effect*). 

Problems with disaggregation (using individual scores and ignoring the group influence) include:

* Resulting analyses ignore group level variables.
* There is often clustering among group members. 
* Clustering (i.e., group effects, dependency in the data) violates the assumption of independence for most ANOVA and regression statistics.
* We are more likely to make a  Type I error (i.e., declaring a statistically significant relationship when there is none) because
  * Alpha inflation
  * Standard error is based on N; standard errors are smaller than they should be.
  * Dependency in the data may reduce within group variance.

### Multilevel modeling: The definitional and conceptual

Multilevel modeling (MLM) has a host of names:

* Hierarchical linear modeling (but this also references a specific, fee-for-use, package)
* Mixed effects
* Linear mixed effects (LME -- you'll see this acronym in our R package and functions)
* Random coefficient regression (RCR)
* Random coefficient modeling (RCM)

By whatever name we call it, the *random coefficient regression model* is an alternative to ordinary least squares regression (OLS) that is structured to handle clustered data. Random coefficient regression differs from OLS regression in the assumptions made about the nature of the regression coefficients and the correlational structure of the individual observations.

Highlights of RC regression models, 

* individuals are clustered into groups 
  * and we can have multiple levels of measurement at the individual and group levels),
* the equations are mathematically different from OLS regression, 
* they can be applied cross-sectional and repeated measures designs. 

In this chapter our focus is on the cross-sectional, nested analyses.

![Image of a three-level model](images/wiGroups/nesting.jpg)

“Levels” on these figures are important and represent the hierarchical structure of RCR.

* Level 1:  lowest level of aggregation, the individual, a micro-level
* Level 2: cluster or group level, the macro-level
* Levels 3 +:  higher-order clustering; beyond the scope of this class (and instructor).

As we work through this chapter we will be reviewing essential elements to MLM.  These include:

* Levels
* Fixed and random effects
* Variance components
* Centering to maximize interpretability and a complete accounting of variance
* Equations

Because these are complicated, it makes sense to me to start introduce the research vignette a little earlier than usual so that we have a concrete example for locating these concepts. First, though, let's look at how we manage an MLM analysis.

## Workflow



## Research Vignette

The research vignette comes from Lefevor et al.'s [-@lefevor_homonegativity_2020] article, "Homonegativity and the Black Church:  Is congregational variation the missing link?"  The article was published in *The Counseling Psychlogist*. I am so grateful to the authors who provided their R script. It was a terrific source of consultation as I planned the chapter.

Data is from congregants in 15 Black churches (with at least 200 members) in a mid-sized city in the South. Congregational participation ranged from 2 to 28. The research design allows the analysts to identify individual level and contextual (i.e., congregational) level predictors of attitudes toward same-sex sexuality.

Variables used in the study included:

* **Attitudes toward Same Sex Sexuality(ATSS)**:  The short form of the Attitudes Toward Lesbian Women and Gay Men Scale [@herek_assessing_1994] is 10 items on a 5-point likert scale of agreement. Sample items include, "Sex between two men is just plan wrong" and "Lesbians are sick." Higher scores represent more homonegative views.

* **Religiousness** Organizational religiousness was assessed through with the single-item organizational religious activity scale of the Duke University Religiousness Index [@koenig_duke_2010]. The item asks participants to report how often they attend church or other religious meetings on a 9-point Likert-type scale ranging from 0 (*never*) to 9 (*several times a week*). Higher scores indicate more frequent attendance.

* **Racial homogeneity** This was calculated by estimating the proportion of respondents from a single race prior to excluding those who did not meet the inclusion criteria (e.g., one criteria was that the participants self-identify as Black).

* **Age, Education, Gender**:  Along with other demographic and background variables, age, education, and gender were collected. Gender is dichotomous with 0 = woman and 1 = man.

In the article, Lefevor [-@lefevor_homonegativity_2020] and colleagues predict attitudes toward same-sex sexuality from a number of person-level (L1) and congregation-level (L2) predictors. Because this is an instructional article, we are choosing one each:  attendance (used as both L1 and L2) and homogeneity of the congregation (L2). Although the authors do not include cross-level (i.e., an interaction between L1 and L2 variables), we will test a cross-level interaction of attendance*homogeneity.

### Simulating the data from the journal article

Muldoon [-@muldoon_simulate_2018] has provided clear and intuitive instructions for simulating multi-level data. 

```{r calculating ranges from article, echo=FALSE, include = FALSE}
#Hidden from the HTML presentation of the OER, this is the calculation of the range of the variables used in the simulation.
# Range of gender
.29 - (.46*3)
.29 + (.46*3)
#Range of age
50.13 - (14.6*3)
50.13 + (14.6*3)
#Range of education
3.96 - (1.5*3)
3.96 + (1.5*3)
# Range of religious attendance (M +/- [3*SD])
7.75 - (.88*3)
7.75 + (.88*3)
# Range of racial homogeneity (M +/- [3*SD])
.91 - (.18*3)
.91 + (.18*3)
```

Simulating the data gives us some information about the nature of MLM.  You can see that we have identified:

* the number of churches
* the number of members from each church
  * Note:  in this simulation we have the benefit of non-missing data (unless we specify it)
* the b weights (and ranges) reported in the Lefevor et al. [-@lefevor_homonegativity_2020] article
* the mean and standard deviation of the dependent variable

Further down in the code, we feed R the regression equation.
```{r simulate raw data}
set.seed(200407)
n_church = 15
n_mbrs = 15
b0 = 3.43 #intercept for ATSS
b1 = .14 #b weight for L1 var gender
b2 = .00 #b weight or L1 var age
b3 = .02 #b weight for L1 var education
b4 = .10 #b weight for the L1 variable religious attendance
b5 = -.89 #b weight for the L2 variable, racial homogeneity
( Gender = runif(n_church*n_mbrs, -1.09, 1.67)) #calc L1 gender
( Age = runif(n_church*n_mbrs, 6.44, 93.93)) #calc L1 age
( Education = runif(n_church*n_mbrs, 0, 8.46)) #calc L1 education
( Attendance = runif(n_church*n_mbrs,5.11, 10.39)) #calc L1 attendance by grabbing  its M +/- 3SD
( Homogeneity = rep (runif(n_church, .37, 1.45), each = n_mbrs)) #calc L2 homogeneity by grabbing  its M +/- 3SD
mu = 3.39 
sds = .64 #this is the SD of the DV
sd = 1 #this is the observation-level random effect variance that we set at 1

( church = rep(LETTERS[1:n_church], each = n_mbrs) )
#( mbrs = numbers[1:(n_church*n_mbrs)] )
( churcheff = rnorm(n_church, 0, sds) )
( churcheff = rep(churcheff, each = n_mbrs) )
( mbrseff = rnorm(n_church*n_mbrs, 0, sd) )
( ATSS = b0 + b1*Gender + b2*Age + b3*Education + b4*Attendance + b5*Homogeneity + churcheff + mbrseff)
( dat = data.frame(church, churcheff, mbrseff, Gender, Age, Education, Attendance, Homogeneity, ATSS) )

library(dplyr)
dat <- dat %>% mutate(ID = row_number())
#moving the ID number to the first column; requires 
dat <- dat%>%select(ID, everything())

Lefevor2020 <- dat%>%
  select(ID, church, Gender, Age, Education, Attendance, Homogeneity, ATSS)
#rounded gender into dichotomous variable
Lefevor2020$Female0 <- round(Lefevor2020$Gender, 0)
Lefevor2020$Female0 <- as.integer(Lefevor2020$Gender)
Lefevor2020$Female0 <- plyr::mapvalues(Lefevor2020$Female0, from = c(-1, 0, 1), to = c(0, 0, 1))

#( dat$ATSS = with(dat, mu + churcheff + mbrseff ) )

```
Because we are simulating data, we have the benefit of no missingness and relatively normal distributions. Because of these reasons we will skip the formal data preparation stage.  We will, though, take a look at our characteristics and bivariate relations of our three variables of interest.

## Working the Problem (and learning MLM)

### Data diagnostics

Multilevel modeling holds assumptions that will likely be familiar to use:

* linearity
* homogeneity of variance
* normal distribution of the model's residuals

Because I cover strategies for evaluating these assumptions in the [Data Dx](https://lhbikos.github.io/ReC_MultivModel/DataDx.html)chapter, I won't review them here.  Another helpul resource for reviewing assumptions related to MLM is provided in by [Michael Palmeri](https://ademos.people.uic.edu/Chapter18.html#6_assumptions).

We should, though take a look at the relations between the variables in our model in their *raw* form.  In this case *raw* refers to their scored, ready-to-be-analyzed (but not further centered).

```{r pairs panels of MLM variables}
library(psych)
psych::pairs.panels(Lefevor2020[c("ATSS", "Attendance", "Homogeneity")], stars = TRUE)
```
This preliminary analysis suggests that 

What do we observe in this preliminary, zero-ordered relationship?

* As racial homogeneity increases, so does homonegativity. 
  * Curiously, there is a non-linear curve between those two variables -- but that seems to be "pulled" by an outlier(?) in the lower right quandrant of the ATSS/Homonegativity relationship.
* ATTS appears to be normally distributed
* Attendance has a flat distribution

We can learn more by examining descriptive statistics.

```{r}
psych::describe(Lefevor2020[c("ATSS", "Attendance", "Homogeneity")])
```

These descriptives allow us a glimpse of the means and standard deviations of our study variables.  Additionally, we can look at skew and kurtosis to see that our variables are within the normal ranges (i.e., below 3 for skew; below 8 for kurtosis [@kline_principles_2016]).

### Levels 

*Levels* are a critical component of MLM. In the context of MLM models of nesting within groups/clusters (e.g., cross-sectional MLM):

* Level 1 (L1) variables "belong to the person"
  * Age, race, attitudinal or behavioral assessment 
* Level 2 (L2) variables "belong to the group/cluster" 
  * Leader characteristic, economic indicator that is unique to the group/cluster
  * Aggregate/composite representation of L1 variables

In our tiny model from the Lefevor [-@lefevor_homonegativity_2020] vignette:

* ATSS is our DV; it is an L1 observation because we are predicting individual's attitudes toward same-sex sexuality.
* Attendance is an L1 observation *when* we are using it as the individual's own church attendance.
* Attendance *will be * an L2 observation when we aggregate it an use it as a value to represent the church.
* Racial homogeneity is only entered as an L2 variable. It was collected at the individual level via self-identification of race and calculated to represent the proportion of Black people in the church. (As noted in the description of the measures, only data from Black participants was included in this paper).

```{r}
head(Lefevor2020[c("church", "ATSS", "Attendance", "Homogeneity")], n = 30L)
```
In this display of the first 30 rows, we see the data for the first two churches (i.e., A and B).  The value is (potentially) different for each individual in each church for the two L1 variables:  *ATSS*, *Attendance.* In contrast, the value of the variable is constant for the L2 variable, *Homogeneity* for churches A and B. 

### Centering

Before we continue with modeling, we need to consider *centering*. That is, we transform our predictor variables to give the intercept parameters more useful interpretations. 

While there are some general practices, there are often arguments for different approaches:

* We usually focus centering on L1 predictors.
* We usually focus centering on continuously scaled variables.
* Dichotomous variables are considered to be centered, so long as there is a meaningful 0 (e.g., control group = 0; treatment group = 1), many do not further center.
  * Newsom [-@newsom_centering_2019], though, argues that if a binary variable is an L1 predictor, group mean centering produces intercepts weighted by the proportion of 1 to 0 values for each group; grand mean centering provides the sample weight adjustment to make the sample mean (each group's mean) proportionate to the population (full sample) 
* Dependent variables are generally not centered

we generally consider three centering strategies:

The **natural metric** is ideal if the variable has a meaningful zero point (e.g., drug dosage, time). It is more difficult when there is a non-zero metric.  When there are dichotomous variables, the natural metric works well (i.e., 0 = control group, 1 = treatment group).  The natural metric is an acceptable choice when the interest is only on the effects of L1 variables, rather than on the effects of group-level variables.

**Grand mean centering (GCM)** involves subtracting the mean from each case's score on the variable.  The intercept is interpreted as the expected value of the DV for a person/group that is compared with all individuals/groups.
 
* Intercepts are adjusted  group means (like an ANCOVA model)
* Variance in the intercepts represents between-group variance in the adjusted means (i.e., adjusted for L1 predictors)
* The effects of L1 predictors are partialed out (controlled for) of the between-group variance
* GCM is most useful when we are interested in
  * L2 predictors with L1 covariates
  * Interactions specified L2
* GCM is a good choice when the primary interest is on the effects of L2 variables, controlling for the L1 predictors.

**Group mean centering** or **Centering within Context (CWC)** involves subtracting the mean of the individual’s group from each score. The L1 intercept is interpreted as the expected mean on the DV for the person’s group. Group mean centering/CWC:

* Provides a measure of the IV that accounts for one’s relative standing within the group
* Removes between-group variability from the model (deviations rom the group means are now the predictors)
  * If we only use group mean centering (CWC), we lose information about between-group differences
* Assumes that relative standing within the group is an important factor
* Is most useful when we are interested in
  * Relations among L1 variables
  * Interactions among L1 variables
  * Interactions between L1 and L2 variables
* CWC is an acceptable choice when the interest is only on the effects of L1 variables, rather than on the effects of group-level variables because it provides unbiased estimates of the pooled within group effect of an individual variable.

In the case of making centering choices with our variables, we are must think about the *frog pond effect*.  That is, for the same size frog, the experience of being in a pond with big frogs may be different from being in a pond with frog ponds.  When we consider our present research vignette, we might ask, 

* Does the effect of church attendance on ATSS depend only on the individual's own church attendance. Or,
* Does the overall church attendance ("size" of the pond) also related to ATSS?

**Compositional effects** [@enders_centering_2007] involves transforming the natural metric of the score into a group-mean centered (CWC) variable at L1 and a group mean **aggregate** at L2. Both the CWC/L1 and aggregate/L2 are entered into the MLM. 

* When the aggregate is added back in at L2, we get *direct* estimates of both the within- and between- group effects through group-mean centering
* We term it *compositional effects* because it represents the difference between the contextual-level effect and the person-level predictor.
* This is a great strategy when the interest is on distinguishing individual effects of variables (e.g., church attendance) from group-level effects of that same variable (e.g., overall church attendance).

Following the Lefevor and colleagues' [-@lefevor_homonegativity_2020] example, we will use the *compositional effects* approach with our data. The *group.center()* function in the R package, *robumeta* will group mean center (CWC) variables.  All we need to do is identify the clustering variable in our case, "church."

Similarly, *robumeta*'s *group.mean* function will aggregate variables at the group's mean.  

```{r centering, warning=FALSE}
library(robumeta)
Lefevor2020$AttendL1 <- as.numeric(group.center(Lefevor2020$Attendance, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$AttendL2 <- as.numeric(group.mean(Lefevor2020$Attendance, Lefevor2020$church))#aggregated at group mean
```

```{r}
head(Lefevor2020[c("church", "ATSS", "Attendance", "AttendL1", "AttendL2", "Homogeneity")], n = 30L)
```

If we look again at the first two churches, we can see the 

* Natural metric (ATSS, Attendance) which differs for each person across all churches
  * This would be an L1 variable
* Group-mean centering (CWC; ATSSL2) which is identifiable because if you added up each of the values in each of the churches, the sum would be zero for each church
  * This would be an L1 variable
* Aggregate group mean (AttendL2) which is identifiable because the value is constant across each of the groups
  * This would be an L2 variable
* You might notice, I didn't mention the *homogeneity* variable.  This is because it was collected and entered as an L2 variable and needs no further centering/transformation. Similarly, we typically leave the dependent variable (*ATSS*) in the natural metric.

We can also see the effects of centering in our descriptives.
```{r}
psych::describe(Lefevor2020[c("ATSS", "Attendance", "AttendL1", "AttendL2", "Homogeneity")])
```
Note that the mean for the ATTSL1 and AttendL1 variables are now zero, while the aggregated group means are equal to the mean of the natural metric.

Looking at the descriptives for each church also helps clarify what we have done.

```{r}
psych::describeBy(ATSS + Attendance + AttendL1 + AttendL2 + Homogeneity ~ church, data = Lefevor2020)
```
Tables are produced for each church's data. Again, because of group-mean centering (CWC) the mean of the ATSSL1 and AttendL1 variables are 0.  The values of the ATSSL2 and AttendL1 variables equal the natural metric. These, though, are different for each of the churches.

Looking at the correlations between all forms of these variables can further help clarify why the *compositional effects* approach is useful.

```{r Multilevel r matrix}
#Multilevel level correlation matrix
apaTables::apa.cor.table(Lefevor2020[c(
"ATSS", "Attendance", "AttendL1", "AttendL2", "Homogeneity")], show.conf.interval = FALSE, landscape = TRUE, table.number = 1, filename="ML_CorMatrix.doc")
```

The AttendL2 (aggregated group means) we created correlates with the Attendance (natural metric) version.  However, it has ZERO correlation with the AttendL1 (group-mean centered, CWC) version. This means that it effectively and completely separtes within- and between-subjects variance.  If we enter these both into the MLM prediction equation, we will completely capture the within- and between-subjects contributions of attendance.

The compositional effects approach to representing L1 variables also works well with longitudinal MLM.

### Model Building

Multilevel modelers often approach model building in a systematic and sequential manner. This approach was true for Lefevor and colleagues [-@lefevor_homonegativity_2020] who planned a four staged approach, but stopped after three because it appeared that adding the next term would not result in model improvement.  The four planned steps include:

* Examining an intercept-only model
* Adding the L1 variables
* Adding the L2 variables
* Adding cross-level interactions

#### Model 1:  The *empty* model

This preliminary model has several names: unconditional cell means model, one-way ANOVA with random effects, intercept-only model, and empty model.  Why?  The only variable in the model is the DV. That is, it is a model with no predictors.

As you can see in the script below, we are specifying its intercept (~1).  The "*(1 | church)*" portion of the code indicates there is a random intercept with a fixed mean. That is, the formula acknowledges that the ATSS means will differ across churches. This model will have no slope.  That is, each individual score is predicted solely from the mean. In another lecture, I talk about the transition from null hypothesis statistical testing to statistical modeling. In that lecture I reflected on Cumming's [-@cumming_new_2014] notion that "even the mean is a model" -- that it explains something and not others.  In this circumstance, the mean is a model!  What is, perhaps, unique about this model is that the code below allows the mean to vary across groups.

There are two packages (*lme4*, *nlme*) that are primarily used for MLM. We are providing the code for both because -- although the core features are identical -- they are slightly different.  The *lmerTest* package offers some handy follow-up tests that help us understand our results.  Finally, the *tab_model()* function from the *sjPlot* package will help create a table that is readily usable in an APA style journal article. 

```{r Mod1 Empty model}
library(lme4)
Mod1 <- lmer(ATSS ~1 + (1 | church), REML = FALSE, data = Lefevor2020)
summary(Mod1)
AIC(Mod1) # request AIC
BIC(Mod1) # request BIC

library(nlme)
ModB1 <- lme(ATSS ~ 1, random = ~ 1|church, method="ML", na.action = na.omit, data = Lefevor2020)
summary(ModB1)
anova(ModB1) # request F-tests for fixed effects

library(lmerTest)
ranova(Mod1) # request test of random effects
confint(Mod1) # request test of random effects (variance displayed as SD)

# Extract Variances to compute R^2
  var_table = as.data.frame(VarCorr(Mod1))
  Mod1_var_tot = var_table[1,'vcov'] + var_table[2,'vcov'] # var_table[1,'vcov'] = L1 var; var_table[2,'vcov'] = L2 var;  

library(sjPlot)
tab_model(Mod1, ModB1, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "ModB1"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

It is customary to report MLM models side-by-side for comparison. In this first run, I have extracted the intercept-only models from both the *lmer()* and *nlme()* runs to show that the results are identical.  In subsequent runs, I will pull from the *lmer()* models.

The unconditional cell means model is equivalent to a one-factor random effects ANOVA of attitudes toward same-sex sexuality as the sole factor; the 15 churches become the 15 levels of the churches factor.

With the *plot_model()* function in *sjPlot*, we can plot the random effects.  For this intercept-only model, we see the mean and range of the ATSS variable
```{r}
library(sjPlot)
sjPlot::plot_model (Mod1, type="re")
```

Focusing on the information in viewer, we can first check to see if things look right.  We know we have 15 churches, each with 15 observations (225), so our data is reading correctly.

The top of the output includes our *fixed effects*. In this case, we have only the intercept, its standard error, and p value. The values of fixed effects do not vary between L2 units. The *tab_model* viewer is very customizable; we can ask for different features.

The section of *random effects* is different from OLS. Random effects include *variance components*; these are reported here.

$\sigma^{2}$ is within-church variance; the pooled scatter of each individual's response around the church's mean.
$\tau _{00}$ is between-church variance; the scatter of each church's data around the grand mean.
The *intraclass correlation coefficient* (ICC) describes the proportion of variance that lies *between* churches. It is the essential piece of data that we need from this model.  Because the total variation in *Y* is just the sum of the within- and between- church variance components, we could have calculated this value from $\sigma^{2}$ and $\tau _{00}$.  Yet, it is handy that the *lmer()* function does it or us.

```{r manual calculation of the ICC}
.27/(1.13+.27)
```
The ICC value of 0.19 means that 19% of the total variation in attitudes toward same-sex sexuality is attributable to differences between churches. The balance (81%) is attributable to within-church variation (or differences in people).

```{r}
1.00 - .19
```
We will monitor these variance components to see if the terms we have added reduce the variance. They can provide some sort of guide as to whether the remaining/unaccounted for variance is within-groups (where an L1 variable could help) or between-groups (where an L2 variable might be indicated). As they approach zero, it could be there is nothing left to explain.

The **deviance statistic** compares log-likelihood statistics for two models at a time:  (a) the current model and (b) a saturated model (e.g., a more general model that fits the sample data perfectly).  Deviance quantifies *how much worse* the current model is  in comparison to the best possible model.  The deviance is identical to the residual sums of squares used in regression.  While you cannot directly interpret any particular deviance statistic, you can compare *nested* models; the smaller value "wins."  The deviance statistic has a number of conditions. After we evaluate several models, we can formally test to if the decrease in deviance statistic is statistically signifcant.

The *AIC* is another fit index. The AIC (Akaike Information Criteria) allows the comparison of the relative *goodness of fit* of models that are not nested.  That is, they can involve different sets of parameters. Like the deviance statistic, the AIC is based on the log-likelihood statistic.  Instead of using the LL itself, the AIC penalizes (e.g., decreases) the LL based on the number of parameters.  Why?  Adding parameters (even if they have no effect) *will* increase the LL statistic and decrease the deviance statistic.  *As long as two models are fit to the identical same set of data*, the AICs can be compared.  The model with the smaller information critera "wins."  There are no established criteria for determining how large the difference is for it to matter.

```{r Mod2 Adding L1 predictor}
# MODEL 2
Mod2 <- lmer(ATSS ~ AttendL1 + (1 | church), REML=FALSE, data = Lefevor2020)
summary(Mod2)
AIC(Mod2) # request AIC
BIC(Mod2) # request BIC
ModB2 <- lme(ATSS ~  AttendL1, random = ~ 1|church, method="ML", na.action = na.omit, data =Lefevor2020)
summary(ModB2)
anova(Mod2) # request F-tests for fixed effects
ranova(Mod2) # request test of random effects
confint(Mod2) # request test of random effects (variance displayed as SD)
anova(Mod1, Mod2) 

# Extract Variances to compute R^2
  var_table = as.data.frame(VarCorr(Mod2))
  Mod2_var_tot = var_table[1,'vcov'] + var_table[2,'vcov'] # var_table[1,'vcov'] = L1 var; var_table[2,'vcov'] = L2 var; 

tab_model(Mod1, Mod2, p.style = "numeric", show.ci = FALSE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("ATSSm1", "ATSSm2"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

```{r}
sjPlot::plot_model (Mod2, type="pred", terms= c("AttendL1"))
```

```{r}
sjPlot::plot_model (Mod2, type="diag")
```


With the *plot_model()* function in *sjPlot*, we can plot the random effects.  For this intercept-only model, we see the mean and range of the ATSS variable

```{r}
# MODEL 3
Mod3 <- lmer(ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church), REML=FALSE, data = Lefevor2020)
summary(Mod3)
AIC(Mod3) # request AIC
BIC(Mod3) # request BIC
ModB3 <- lme(ATSS ~  AttendL1 +  AttendL2 + Homogeneity, random = ~ 1|church, method="ML", na.action = na.omit, data =Lefevor2020)
summary(Mod3)
anova(Mod3) # request F-tests for fixed effects
ranova(Mod3) # request test of random effects
confint(Mod3) # request test of random effects (variance displayed as SD)
anova(Mod1, Mod2, Mod3) 

tab_model(Mod1, Mod2, Mod3, p.style = "numeric", show.ci = FALSE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2", "Mod3"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

```{r}
sjPlot::plot_model (Mod3, type="pred")
```

```{r}
sjPlot::plot_model (Mod3, type="pred",terms=c("AttendL1", "Homogeneity", "AttendL2"))
```

```{r}
sjPlot::plot_model (Mod3, type="diag")
```


```{r}
# MODEL 4
Mod4 <- lmer(ATSS ~ AttendL2 + AttendL1*Homogeneity +(1 | church), REML=FALSE, data = Lefevor2020)
summary(Mod4)
AIC(Mod4) # request AIC
BIC(Mod4) # request BIC
ModB3 <- lme(ATSS ~  AttendL2 + AttendL1*Homogeneity, random = ~ 1|church, method="ML", na.action = na.omit, data =Lefevor2020)
summary(Mod4)
anova(Mod4) # request F-tests for fixed effects
ranova(Mod4) # request test of random effects
confint(Mod4) # request test of random effects (variance displayed as SD)
anova(Mod1, Mod2, Mod3, Mod4) 

tab_model(Mod1, Mod2, Mod3, Mod4, p.style = "numeric", show.ci = FALSE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2", "Mod3", "Mod4"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
```{r}
sjPlot::plot_model (Mod4, type="pred",terms=c("AttendL1", "Homogeneity", "AttendL2"), mdrt.values = "meansd")
```

```{r}
sjPlot::plot_model (Mod4, type="int", terms=c("AttendL1", "AttendL2", "Homogeneity"), mdrt.values = "meansd")
```

```{r}
sjPlot::plot_model (Mod4, type="diag")
```











Finally, the formulae (yes, plural)  

Recall the simplicity of the OLS regresion equation with a single predictor:

$$Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}$$ 
Where:

* $\beta_{0}$ is the population *intercept*
* $\beta_{1}x_{i}$ is the population undstandardized regression *slope*
* $\epsilon_{i}$ is the random error in prediction for case *i*

The outcome $Y_{i}$ has a subscript "$i$", indicating that it is predicted for each individual. 

On the right side of the equation, we see that $Y_{i}$ is predicted by the individual's value on predictor variable $x$. The variable $x$ is a *linear* predictor of $Y$. You might be able to recognize that this equation resembles the equation of a line ($Y = mx+ b$). 

In regression, the linear equation includes an intercept ($\beta_{0}$) and a slope ($\beta_{1}$) parameter, as well as a residual error term ($\epsilon_{i}$) which represents individual uniqueness that is not explained by the model. The uniquenesses, ($\epsilon_{i}$), are typically assumed to follow a normal distirbution with mean $0$ and variance $\sigma^2$, or in statistical terms, $\epsilon_{i}$ ~$N(0, \sigma^2)$.

Importantly, the intercept and slope are both *fixed* in a basic linear regression model. You can recognize that the intercept and slope are fixed because they do not include a subscript $i$ or $j$ (as we will see later). The lack of a subscript indicates that these parameters each only take on a single value that is meant to represent the entire population intercept or slope, respectively. 

Now let's take a look at the most basic **multilevel model** and compare it to the simple linear regression model above:

$$ Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij} $$
Where…

*	$Y_{ij}$ outcome measure for individual *i* in group *j*
–	$X_{ij}$ is the value of the predictor for individual *i* in group *j*
–	$\beta_{0j}$ is the intercept for group *j*
–	$\beta_{1j$ is the slope for group *j*
–	$\epsilon_{ij}$ is the residual 

Does it look familiar? The only difference in between this MLM equation and the linear regression equation is that the MLM equation contains more subscripts. As you will see, this *simple* update will give us so much more information. For the purpose of defining the model, let's assume that the subscript $j$ represents a group of individuals. In the multilevel model, $i$ can take on any value in $(1, ..., N)$, where $N$ is the number of individuals in the study. The subscript $j$ may take on values in $(1, ..., J)$, where $J$ is the number of groups in the study. In this model, recognize that each group is allowed its own unique intercept and slope. You could read the entire model as: 

*"The outcome value for person $i$ in group $j$ is equal to the intercept for group $j$, plus the slope for group $j$ multiplied by the x-value for person $i$ in group $j$, plus some error that cannot be explained by the model for person $i$ in group $j$."*

The errors in $\epsilon_{ij}$ are typically assumed to be independently and identically distributed $(iid)$ ~$N(0, \sigma^2)$. 

Level 2 (macro-level) regression equations carry the group structure.

$$\beta _{0j}=\gamma _{00}+\mu _{0j}$$
$$\beta _{1j}=\gamma _{10}+\mu_{1j}$$

$\beta _{0j}$ models the differences in the group intercepts, predicting the intercept for group *j*

* $\gamma _{00}$ is the population regression intercept
  * the grand mean
* $\gamma _{00}$ assesses how much group *j* differs from the grand mean
  * a measure of variance

$beta _{1j}$ models the differences in group slopes

* $\gamma _{10}$ is a fixed or constant population slopes
* $u_{1j}$ assesses the extent to which group *j*'s slope differs from the *grand slope*.

$\mu _{0j}$ and $\mu_{1j}$ are the residuals from trying to predict the intercepts and slopes, respectively.


So far it looks like the level 1 and level 2 equations are treated separately. We combine them to form a single *multi-level* regression equation referred to as the "mixed model."  This is Cohen et al.'s [-@cohen_applied_2003] rendition:  

$$Y_{ij}=\gamma _{10}X_{ij}+\gamma _{00}+U_{0j}+U_{ij}X_{ij}+r_{ij}$$

### Variance components

Another augmentation from OLS is the utilization of *variance components*.  Considered to be a hallmark of multi-level models, these summarize the differences among the groups. The following are used to identify and estimate the source of random errors/deviations:

* $\sigma ^{2}$: L1 (within subjects) random errors ($r_{ij}$), from random variation in the *Y* scores
* $\tau _{00}$: L2 (between subjects) deviations of the random intercepts around the population intercept, $\mu_{0j}$
* $\tau _{11}$: L2 (between subjects) deviations of the random slopes around the population slope, $\mu_{1j}$
* $\tau _{01}$: covariance between the errors of the random regression coefficients and the random regression intercepts (i.e,. covariance between $\mu_{0j}$ and $\mu_{1j}$; the L1 slopes and intercepts across groups).

#### Could a multi-level model ever simplify to the fixed OLS model? Yes, it could

In OLS regression there is only 1 variance component, $\sigma ^{2}$, however in MLM there are three (or four) variance components.  If $\tau _{00}$ and $\tau _{11}$ are equal to zero in the population, then the MLM model simplifies to fixed OLS regression.  This would happen:

* If all the intercepts of all the L1 regression equations were identical, then $\tau _{00}$ = 0 (no variance among intercepts).
*	If the slopes of all L1 regression equations in all groups were identical, then c = 0 (no variance among slopes).
* If both $\tau _{00}$ and $\tau _{00}$ are zero, there is no effect of clustering/group membership and the RCR equation is equivalent to an OLS regression that ignores group membership.

### Fixed and random effects

Another hallmark of MLM is the distinction between fixed and random effects. Again, let's consider the simplicity of the OLS regression equation: $Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}$

Remember how hard you worked to memorize definitions that included the word, *random*? *Random variables* are those where values are selected at random from a probability distribution, both the error term $\epsilon_i$ and the dependent variable $Y$are random variables.  We assume a normal probability distribution o errors in the population with mean $\mu=0$ and variance $\sigma_{\epsilon}^{2}$

In OLS regression, the Xs are fixed (i.e., they take on a predetermined set of values). Additionally, the $\beta_{0}$ and $\beta_{1$ are fixed parameters of the unstandardized regression equation for the whole population regression equation; the $B_1$ and $B_0$ in the sample equation are fixed.

In MLM, *random effects* (L1 regression coefficients) are those that are allowed to vary between L2 units.  In contrast, *fixed effects* (L2 regression coefficients) do not vary. Stated another way, *fixed effects* are single values applied to each L1 unit regardless of the L2 unit under which a case is nested.In the mixed model equation we look at the fixed effects to tell the story of the intercept and slope.









library(sjPlot)
plot_model (ModD_lme4, type="int", terms=c("age_14", "coa [0,1]", "peer [.77, 1.02, 1.75]"))

### Quick peek at the data

Descriptives
R matrix
Consider pairpanels
Consider the M, SD, r matrix from apaTables


## Working the NAME OF STATISTIC

Method/Analytic Strategy APA style description of the hypotheses and how they will be analyzed.

![relevant conceptual or statistical figure of the planned analysis](images/subfolder/filename.jpg)

Relevant formulas

$$M = i_{M}+a_{1}X + a_{2}W + a_{3}XW + e_{M}$$

$$Y = i_{Y}+c_{1}^{'}X+ c_{2}^{'}W+c_{3}^{'}XW+ bM+e_{Y}$$

### Analysis

If possible, work in smaller sections.  Provide APA style results after each meaningful chunk (that will be assembled into the formal results, later).


```{r Placeholder to write objects to an outfile}
#write.csv (objectname, file="filename.csv") #optional to write it to a .csv file
```

```{r Placeholder to create model object names, echo = FALSE, results ='hide'}
library(formattable) #to use the digits function
# Values for the intercept of the IV
#Yicpt <- digits(cParamEsts$est[9], 3) #B weight for the intercept
##Yicpt 
#Yicpt_SE <- digits(cParamEsts$se[9], 3)#p value for the intercept
#Yicpt_SE
#Yicpt_p <- digits(cParamEsts$pvalue[9], 3)#p value for the intercept
#Yicpt_p

# Proportion of variance accounted for
#Rsq_Y <- percent(cFITsum$PE$est[30])
#Rsq_Y
#Rsq_M <- percent(cFITsum$PE$est[29])
#Rsq_M
```

### Beginning the interpretation

Placeholder for formula

$$\hat{M} = i_{M}+a_{1}X + a_{2}W + a_{3}XW + e_{M}$$


$$\hat{Y} = i_{Y}+c_{1}^{'}X+ c_{2}^{'}W+c_{3}^{'}XW+ bM+e_{Y}$$

And substitute in our values

$$\hat{M} = 1.417 + 0.212X + (-0.027) W + 0.006XW$$
$$\hat{Y} = 31.703 + (-1.4115)X + (-0.556)W + 0.164XW + (-3.567)M$$

### Tabling the data

Table 1  

|Placeholder for table
|:----------------------------------------------------------------------------------------------------------------------------|

|                         
|:------------------|:--------------------------------------------------:|:--------------------------------------------------:|
|                   |Disengagement Coping (M)                            |Mental Health (Y)                                   |

|
|:------------------|:-----:|:-----------:|:-------------:|:------------:|:-----:|:----------:|:-------------:|:-------------:|
|Antecedent         |path   |$B$          |$SE$           |$p$           |path   |$B$         |$SE$           |$p$            |
|constant           |$i_{M}$|`r MM_Micpt` |`r MM_Micpt_SE`|`r MM_Micpt_p`|$i_{Y}$|`r MM_Yicpt`|`r MM_Yicpt_SE`|`r MM_Yicpt_p` |
|GRMS (X)           |$a_{1}$|`r MMa1`     |`r MMa1_SE`    |`r MMa1_p`    |$c_{1}$|`r MMc_p1`  |`r MMc_p1_SE`  |`r MMc_p1_p`   |
|GRIcntrlty (W)     |$a_{2}$|`r MMa2`     |`r MMa2_SE`    |`r MMa2_p`    |$c_{2}$|`r MMc_p2`  |`r MMc_p2_SE`  |`r MMc_p2_p`   |
|GRMS*GRIcntrlty(XW)|$a_{3}$|`r MMa3`     |`r MMa3_SE`    |`r MMa3_p`    |$c_{3}$|`r MMc_p3`  |`r MMc_p3_SE`  |`r MMc_p3_p`   |
|DisEngmt (M)       |       |             |               |              |$b$    |`r MMb`     |`r MMb_SE`     |`r MMb_p`      |

|
|:------------------|:--------------------------------------------------:|:--------------------------------------------------:|
|                   |$R^2$ = `r MM_Rsq_M`                                |$R^2$ = `r MM_Rsq_Y`                                |

|                         
|:---------------------------------------------------------------------------------------------------------------------------:|
|Conditional Indirect, Direct, and Total Effects at Gendered Racial Identity Centrality Values                                |       

|                         
|:----------------------------|:--------------:|:-----------:|:--------------:|:---------------------------------------------:|
|                             |Boot effect     |Boot SE      |Boot CI95 lower |Boot CI95 upper                                |
|Index of moderated mediation |`r MMimm`       |`r MMimm_SE` |`r MMimm_CIloL` |`r MMimm_CIhiL`                                |
|Indirect                     |                |             |                |                                               |
|-1 *SD*                      |`r MMindL`      |`r MMind_seL`|`r MMind_CIloL` |`r MMind_CIhiL`                                |
|Mean                         |`r MMindM`      |`r MMind_seM`|`r MMind_CIloM` |`r MMind_CIhiM`                                |
|+1 *SD*                      |`r MMindH`      |`r MMind_seH`|`r MMind_CIloH` |`r MMind_CIhiH`                                |
|Direct                       |                |             |                |                                               |
|-1 *SD*                      |`r MMdirL`      |`r MMdir_seL`|`r MMdir_CIloL` |`r MMdir_CIhiL`                                |
|Mean                         |`r MMdirM`      |`r MMdir_seM`|`r MMdir_CIloM` |`r MMdir_CIhiM`                                |
|+1 *SD*                      |`r MMdirH`      |`r MMdir_seH`|`r MMdir_CIloH` |`r MMdir_CIhiH`                                |
|Total                        |                |             |                |                                               |
|-1 *SD*                      |`r MMtotL`      |`r MMtotL_se`|`r MMtot_CIloL` |`r MMtot_CIhiL`                                |
|Mean                         |`r MMtotM`      |`r MMtotM_se`|`r MMtot_CIloM` |`r MMtot_CIhiM`                                |
|+1 *SD*                      |`r MMtotH`      |`r MMtotH_se`|`r MMtot_CIloH` |`r MMtot_CIhiH`                                |

|
|-----------------------------------------------------------------------------------------------------------------------------|
*Note*. .


### APA Style Writeup

**Method/Analytic Strategy**


**Results**

**Preliminary Analyses**

*  Missing data anlaysis and managing missing data
*  Bivariate correlations, means, SDs
*  Distributional characteristics, assumptions, etc.
*  Address limitations and concerns

**Primary Analyses**


## Residual and Related Questions...

..that you might have; or at least I had, but if had answered them earlier it would have disrupt the flow.

   
## Practice Problems

The suggested practice problem for this chapter is to conduct a simple mediation.


### Problem #1: Rework the research vignette as demonstrated, but change the random seed

If this topic feels a bit overwhelming, simply change the random seed in the data simulation, then rework the problem. This should provide minor changes to the data (maybe in the second or third decimal point), but the results will likely be very similar.

|Assignment Component  
|:---------------------------------------------------------------------------------------------|:-------------: |:------------:|
|1. Assign each variable to the X, Y, M, or W roles (ok but not required  to include a cov)    |      5         |    _____     |      
|2. Specify and run the lavaan model                                                           |      5         |    _____     |
|3. Use semPlot to create a figure                                                             |      5         |    _____     |
|4. Create a table that includes regression output for the M and Y variables and the moderated effects| 5       |    _____     |  
|5. Represent your work in an APA-style write-up                                               |      5         |    _____     |          
|6. Explanation to grader                                                                      |      5         |    _____     |   
|7. Be able to hand-calculate the indirect, direct, and total effects from the a, b, & c' paths|      5         |    _____     |
|**Totals**                                                                                    |      35        |    _____     |

### Problem #2:  Rework the research vignette, but swap one or more variables

Use the simulated data, but select one of the other models that was evaluated in the Lewis et al. [@lewis_applying_2017] study.  Compare your results to those reported in the mansucript.

|Assignment Component  
|:---------------------------------------------------------------------------------------------|:-------------: |:------------:|
|1. Assign each variable to the X, Y, M, or W roles (ok but not required  to include a cov)    |      5         |    _____     |      
|2. Specify and run the lavaan model                                                           |      5         |    _____     |
|3. Use semPlot to create a figure                                                             |      5         |    _____     |
|4. Create a table that includes regression output for the M and Y variables and the moderated effects|      5  |    _____     |  
|5. Represent your work in an APA-style write-up                                               |      5         |    _____     |          
|6. Explanation to grader                                                                      |      5         |    _____     |   
|7. Be able to hand-calculate the indirect, direct, and total effects from the a, b, & c' paths|      5         |    _____     |
|**Totals**                                                                                    |      35        |    _____     |
             
                                                                 

### Problem #3:  Use other data that is available to you

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a simple mediation.

|Assignment Component  
|:---------------------------------------------------------------------------------------------|:-------------: |:------------:|
|1. Assign each variable to the X, Y, M, or W roles (ok but not required  to include a cov)   |      5          |    _____     |      
|2. Specify and run the lavaan model                                                           |      5         |    _____     |
|3. Use semPlot to create a figure                                                             |      5         |    _____     |
|4. Create a table that includes regression output for the M and Y variables and the moderated effects |5       |    _____     |  
|5. Represent your work in an APA-style write-up                                               |      5         |    _____     |          
|6. Explanation to grader                                                                      |      5         |    _____     |   
|7. Be able to hand-calculate the indirect, direct, and total effects from the a, b, & c' paths|      5         |    _____     |
|**Totals**                                                                                    |      35        |    _____     |
   

## Bonus Track: 

![Image of a filmstrip](images/film-strip-1.jpg){#id .class width=620 height=211}

### Working the Entire Vignette

```{r}
library(psych)
psych::pairs.panels(Lefevor2020[c("ATSS", "Female0", "Age", "Education", "Attendance", "Homogeneity")], stars = TRUE)
```
```{r}
psych::describe(Lefevor2020[c("ATSS", "Female0", "Age", "Education", "Attendance", "Homogeneity")])
```

```{r Single level r matrix}
#Single level correlation matrix
apaTables::apa.cor.table(Lefevor2020[c(
"ATSS", "Female0", "Age", "Education", "Attendance", "Homogeneity")], show.conf.interval = FALSE, landscape = TRUE, table.number = 1, filename="CorMatrix.doc")
```

```{r centering, warning=FALSE}
library(robumeta)
Lefevor2020$ATSSL1 <- as.numeric(group.center(Lefevor2020$ATSS, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$ATSSL2 <- as.numeric(group.mean(Lefevor2020$ATSS, Lefevor2020$church))#aggregated at group mean
Lefevor2020$AttendL1 <- as.numeric(group.center(Lefevor2020$Attendance, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$AttendL2 <- as.numeric(group.mean(Lefevor2020$Attendance, Lefevor2020$church))#aggregated at group mean
Lefevor2020$AgeL1 <- as.numeric(group.center(Lefevor2020$Age, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$AgeL2 <- as.numeric(group.mean(Lefevor2020$Age, Lefevor2020$church))#aggregated at group mean
Lefevor2020$GenderL1 <- as.numeric(group.center(Lefevor2020$Female0, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$GenderL2 <- as.numeric(group.mean(Lefevor2020$Female0, Lefevor2020$church))#aggregated at group mean
Lefevor2020$EducL1 <- as.numeric(group.center(Lefevor2020$Education, Lefevor2020$church))#centered within context (group mean centering)
Lefevor2020$EducL2 <- as.numeric(group.mean(Lefevor2020$Education, Lefevor2020$church))#aggregated at group mean
```

CALCULATE L1 AND L2 ATTS
```{r Multilevel r matrix}
#Multilevel level correlation matrix
apaTables::apa.cor.table(Lefevor2020[c(
"ATSSL1", "GenderL1", "AgeL1", "EducL1", "AttendL1",
"ATSSL2","GenderL2", "AgeL2", "EducL2", "AttendL2", "Homogeneity")], show.conf.interval = FALSE, landscape = TRUE, table.number = 1, filename="ML_CorMatrix.doc")
```

```{r}
library(lme4)

ATSSm1 <- lmer(ATSS ~1 + (1 | church), REML = FALSE, data = Lefevor2020)
summary(ATSSm1)
AIC(ATSSm1) # request AIC
BIC(ATSSm1) # request BIC

library(nlme)
ATSSmb1 <- lme(ATSS ~ 1, random = ~ 1|church, method="ML", na.action = na.omit, data = Lefevor2020)
summary(ATSSmb1)
anova(ATSSmb1) # request F-tests for fixed effects

library(lmerTest)
ranova(ATSSm1) # request test of random effects
confint(ATSSm1) # request test of random effects (variance displayed as SD)

# Extract Variances to compute R^2
  var_table = as.data.frame(VarCorr(ATSSm1))
  ATSSm1_var_tot = var_table[1,'vcov'] + var_table[2,'vcov'] # var_table[1,'vcov'] = L1 var; var_table[2,'vcov'] = L2 var;  

library(sjPlot)

tab_model(ATSSm1, ATSSmb1, p.style = "stars", show.ci = TRUE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("ATSSm1", "ATSSmb1"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

```{r}
# MODEL 2
ATSSm2 <- lmer(ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + (1 | church), REML=FALSE, data = Lefevor2020)
summary(ATSSm2)
AIC(ATSSm2) # request AIC
BIC(ATSSm2) # request BIC
ATSSmb2 <- lme(ATSS ~  GenderL1 + AgeL1 + EducL1 + AttendL1, random = ~ 1|church, method="ML", na.action = na.omit, data =Lefevor2020)
summary(ATSSmb2)
anova(ATSSm2) # request F-tests for fixed effects
ranova(ATSSm2) # request test of random effects
confint(ATSSm2) # request test of random effects (variance displayed as SD)
anova(ATSSmb1, ATSSmb2) 

# Extract Variances to compute R^2
  var_table = as.data.frame(VarCorr(ATSSm2))
  m2_var_tot = var_table[1,'vcov'] + var_table[2,'vcov'] # var_table[1,'vcov'] = L1 var; var_table[2,'vcov'] = L2 var; 

tab_model(ATSSm1, ATSSm2, p.style = "numeric", show.ci = FALSE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("ATSSm1", "ATSSm2"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

```{r}
# MODEL 3
ATSSm3 <- lmer(ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + GenderL2 + AgeL2 + EducL2 + AttendL2 + Homogeneity + (1 | church), REML=FALSE, data = Lefevor2020)
summary(ATSSm3)
AIC(ATSSm3) # request AIC
BIC(ATSSm3) # request BIC
#running the nlme ogtained an error indicating there is singularity in the variables
#the stack exchange conversation referenced below indicates that nlme/lmer is sensitive to this
# https://stackoverflow.com/questions/50505290/singularity-in-backsolve-at-level-0-block-1-in-lme-model 
#ATSSmb3 <- lme(ATSS ~  Female0L1 + AgeL1 + EducL1 + AttendL1 + Female0L2 + AgeL2 + EducL2 + AttendL2 + Homogeneity, random = ~ 1|church, method="ML", na.action = na.omit, data =Lefevor2020)
summary(ATSSm3)
anova(ATSSm3) # request F-tests for fixed effects
ranova(ATSSm3) # request test of random effects
confint(ATSSm3) # request test of random effects (variance displayed as SD)
anova(ATSSm1, ATSSm2, ATSSm3) 

tab_model(ATSSm1, ATSSm2, ATSSm3, p.style = "numeric", show.ci = FALSE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("ATSSm1", "ATSSm2", "ATSSm3"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
### Just the Code Please



```{r sessionInfo modmed}
sessionInfo()
```

# References {-#refs}
